{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/AI-ipynb/blob/master/20NN2L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbCs-NMNliBk"
      },
      "source": [
        "## Two Layered Neural Network Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOu1j-zeliBo"
      },
      "source": [
        "The network below is called as 2-layer Neural Network (NN). The input layer is not counted by convention to nomenclate the network. The layers between output and input layers are called as hidden layer. In the NN shown below is having one hidden layer and the output layer.    \n",
        "<img src=\"https://vijayonline.in/images/nn2L.png\" width=\"70%\">\n",
        "\n",
        "We have developed the basic understanding of the processing happening in the neuron or hidden unit. Each unit is supposed to perform two tasks. First, it represnt the input featues and the weights as linear combination and this computation is called as the linear part of hidden unit computaion. The other part of the computation is the activation using non-linear activation function. This can be represneted in general by the following:    \n",
        "<img src=\"https://vijayonline.in/images/model_lr.png\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBv6ga6kliBp"
      },
      "source": [
        "Input Feature vector for $i^{th}$ training example:\n",
        "$\\mathbf{x}^{(i)} = \\mathbf{a}^{[0](i)} = \\begin{pmatrix}\n",
        "  {x}_1^{(i)} \\\\ {x}_2^{(i)} \\\\ \\vdots \\\\ {x}_{nx}^{(i)}\n",
        " \\end{pmatrix}=\\begin{pmatrix}\n",
        "  {a}_1^{[0](i)} \\\\ {a}_2^{[0](i)} \\\\ \\vdots \\\\ {a}_{nx}^{[0](i)}\n",
        " \\end{pmatrix} $   \n",
        "\n",
        "Input Feature vector of the problem dataset:   \n",
        "\n",
        "$ \\mathbf{X} = \\mathbf{A}^{[0]}= \\begin{pmatrix}\n",
        "\\mathbf{x}^{(1)} & \\mathbf{x}^{(2)} & \\cdots & \\mathbf{x}^{(i)}\n",
        "\\end{pmatrix}$   \n",
        "\n",
        "$ \\mathbf{X} = \\mathbf{A}^{[0]} = \\begin{pmatrix}\n",
        "{x}_1^{(1)} & {x}_1^{(2)} & \\cdots & {x}_1^{(m)} \\\\ \n",
        "{x}_2^{(1)} & {x}_2^{(2)} & \\cdots & {x}_2^{(m)} \\\\ \n",
        "\\vdots & \\vdots & \\cdots & \\vdots \\\\ \n",
        "{x}_{nx}^{(1)} & {x}_{nx}^{(1)} & \\cdots & {x}_{nx}^{(m)}\n",
        "\\end{pmatrix}, \\mathbf{X} \\in \\mathbf R ^{nx \\times m}$  \n",
        "\n",
        "**Let the input features be 3 to develop the intuition about the forward and backward propogation.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_afENyeliBq"
      },
      "source": [
        "\n",
        "## Hidden Layer-1 Parameters :  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYS_bAtliBu"
      },
      "source": [
        "\n",
        "Considering input features as activation of zeroth layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-1  Node-1 \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-L1N1.png\" width=\"50%\">\n",
        "\n",
        "$ z^{[1](i)}_1 = w_{11}^{[1]}x^{(i)}_1+w_{12}^{[1]}x^{(i)}_2+w_{13}^{[1]}x^{(i)}_3 + b^{[1]}_1 $   \n",
        "\n",
        "$ z^{[1](i)}_1 = \\mathbf{w}_1^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_1 $  \n",
        "\n",
        "$ z^{[1](i)}_1 = \\mathbf{w}_1^{[1]T}\\mathbf{a}^{[0](i)} + b^{[1]}_1 $  \n",
        "\n",
        "$ a_1^{[1](i)}=g (z^{[1](i)}_1) $\n",
        "\n",
        "$ \\mathbf{w}^{[1]}_1 = \\begin{pmatrix} \n",
        "w_{11}^{[1]} \\\\ w_{12}^{[1]} \\\\ w_{13}^{[1]}\n",
        " \\end{pmatrix} $\n"
      ],
      "metadata": {
        "id": "gaje5iX5V5oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-1  Node-2 \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-L1N2.png\" width=\"50%\">  \n",
        "\n",
        "$ z^{[1](i)}_2 = w_{21}^{[1]}x^{(i)}_1+w_{22}^{[1]}x^{(i)}_2+w_{23}^{[1]}x^{(i)}_3 + b^{[1]}_2 $   \n",
        "\n",
        "$ z^{[1](i)}_2 = \\mathbf{w}_2^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_2 $   \n",
        "\n",
        "$ z^{[1](i)}_2 = \\mathbf{w}_2^{[1]T}\\mathbf{a}^{[0](i)} + b^{[1]}_2 $   \n",
        "\n",
        "$a_2^{[1](i)}=\\sigma (z^{[1](i)}_2) $\n",
        "\n",
        "$ \\mathbf{w}^{[1]}_2 = \\begin{pmatrix} \n",
        "w_{21}^{[1]} \\\\ w_{22}^{[1]} \\\\ w_{23}^{[1]}\n",
        " \\end{pmatrix} $\n"
      ],
      "metadata": {
        "id": "_1ZC26EkQ_xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-1  Node-3 \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-L1N3.png\" width=\"50%\">.  \n",
        "$ z^{[1](i)}_3 = w_{31}^{[1]}x^{(i)}_1+w_{32}^{[1]}x^{(i)}_2+w_{33}^{[1]}x^{(i)}_3 + b^{[1]}_3 $   \n",
        "\n",
        "$ z^{[1](i)}_3 = \\mathbf{w}_3^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_3 $  \n",
        "\n",
        "$ z^{[1](i)}_3 = \\mathbf{w}_3^{[1]T}\\mathbf{a}^{[0](i)} + b^{[1]}_3 $  \n",
        "\n",
        "$a_3^{[1](i)}=\\sigma (z^{[1](i)}_3) $\n",
        "\n",
        "$ \\mathbf{w}^{[1]}_3 = \\begin{pmatrix} \n",
        "w_{31}^{[1]} \\\\ w_{32}^{[1]} \\\\ w_{33}^{[1]}\n",
        " \\end{pmatrix} $\n"
      ],
      "metadata": {
        "id": "81aKKmKLRDx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-1  Node-4 \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-L1N4.png\" width=\"50%\">.  \n",
        "\n",
        "$ z^{[1](i)}_4 = w_{41}^{[1]}x^{(i)}_1+w_{42}^{[1]}x^{(i)}_2+w_{43}^{[1]}x^{(i)}_3 + b^{[1]}_4 $   \n",
        "\n",
        "$ z^{[1](i)}_4 = \\mathbf{w}_4^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_4 $  \n",
        "\n",
        "$ z^{[1](i)}_4 = \\mathbf{w}_4^{[1]T}\\mathbf{a}^{[0](i)} + b^{[1]}_4 $  \n",
        "\n",
        "$a_4^{[1](i)}=\\sigma (z^{[1](i)}_4) $\n",
        "</td>\n",
        "<td>\n",
        "$ \\mathbf{w}^{[1]}_4 = \\begin{pmatrix} \n",
        "w_{41}^{[1]} \\\\ w_{42}^{[1]} \\\\ w_{43}^{[1]}\n",
        " \\end{pmatrix} $</td>\n",
        "</tr></table>\n"
      ],
      "metadata": {
        "id": "kd5-qKYkRG_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writting the linear part of the hidden layer to develop the vectorized form.  \n",
        "$ z^{[1](i)}_1 = w_{11}^{[1]}x^{(i)}_1+w_{12}^{[1]}x^{(i)}_2+w_{13}^{[1]}x^{(i)}_3 + b^{[1]}_1 $   \n",
        "\n",
        "$ z^{[1](i)}_2 = w_{21}^{[1]}x^{(i)}_1+w_{22}^{[1]}x^{(i)}_2+w_{23}^{[1]}x^{(i)}_3 + b^{[1]}_2 $   \n",
        "\n",
        "$ z^{[1](i)}_3 = w_{31}^{[1]}x^{(i)}_1+w_{32}^{[1]}x^{(i)}_2+w_{33}^{[1]}x^{(i)}_3 + b^{[1]}_3 $   \n",
        "\n",
        "$ z^{[1](i)}_4 = w_{41}^{[1]}x^{(i)}_1+w_{42}^{[1]}x^{(i)}_2+w_{43}^{[1]}x^{(i)}_3 + b^{[1]}_4 $   \n"
      ],
      "metadata": {
        "id": "SMm0IUQlaTwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write the above equations in matrix form as below\n",
        "\n",
        "$\\begin{pmatrix}\n",
        "  {z}_1^{[1](i)} \\\\ {z}_2^{[1](i)} \\\\ z_3^{[1](i)} \\\\ {z}_{4}^{[1](i)}\n",
        " \\end{pmatrix}=\\begin{pmatrix}\n",
        "  w_{11}^{[1]} & w_{12}^{[1]} & w_{13}^{[1]} \\\\ \n",
        "  w_{21}^{[1]} & w_{22}^{[1]} & w_{23}^{[1]} \\\\\n",
        "  w_{31}^{[1]} & w_{32}^{[1]} & w_{33}^{[1]} \\\\\n",
        "  w_{41}^{[1]} & w_{42}^{[1]} & w_{43}^{[1]} \n",
        "  \\end{pmatrix}\\begin{pmatrix}\n",
        "  {x}_1^{(i)} \\\\ {x}_2^{(i)} \\\\ {x}_{3}^{(i)}\n",
        " \\end{pmatrix} + \\begin{pmatrix}\n",
        "  {b}_1^{[1]} \\\\ {b}_2^{[1]} \\\\ b_3^{[1]} \\\\ {b}_{4}^{[1]}\n",
        " \\end{pmatrix}$ "
      ],
      "metadata": {
        "id": "Knt-QZtAvmj3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7va57lXliBr"
      },
      "source": [
        "Let us define the vectors\n",
        "\n",
        "$\\mathbf{z}^{[1](i)} = \\begin{pmatrix}\n",
        "  {z}_1^{[1](i)} \\\\ {z}_2^{[1](i)} \\\\ z_3^{[1](i)} \\\\ {z}_{4}^{[1](i)}\n",
        " \\end{pmatrix}; \\mathbf{W}^{[1]} = \\begin{pmatrix}\n",
        "  w_{11}^{[1]} & w_{12}^{[1]} & w_{13}^{[1]} \\\\ \n",
        "  w_{21}^{[1]} & w_{22}^{[1]} & w_{23}^{[1]} \\\\\n",
        "  w_{31}^{[1]} & w_{32}^{[1]} & w_{33}^{[1]} \\\\\n",
        "  w_{41}^{[1]} & w_{42}^{[1]} & w_{43}^{[1]} \n",
        "  \\end{pmatrix}; \\mathbf{x}^{(i)} = \\begin{pmatrix}\n",
        "  {x}_1^{(i)} \\\\ {x}_2^{(i)} \\\\ {x}_{3}^{(i)}\n",
        " \\end{pmatrix}; \\mathbf{b}^{[1]} = \\begin{pmatrix}\n",
        "  {b}_1^{[1]} \\\\ {b}_2^{[1]} \\\\ b_3^{[1]} \\\\ {b}_{4}^{[1]}\n",
        " \\end{pmatrix}$  \n",
        " \n",
        " $\\mathbf W^{[1]}.\\text shape()=(n_h, n_h-1)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DqL8Q9liBr"
      },
      "source": [
        "The vectorized equation can be written as:  \n",
        "\n",
        "$\\mathbf{z}^{[1](i)} = \\mathbf{W}^{[1]} \\mathbf{x}^{(i)} + \\mathbf{b}^{[1]}$   \n",
        "\n",
        "The output of the linear part is then activated by a suitable activation function.   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above vectorised equation can also be developed by considering the output of the linear part in vector form.   \n",
        "\n",
        "$ z^{[1](i)}_1 = \\mathbf{w}_1^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_1 $.  \n",
        "\n",
        "$ z^{[1](i)}_2 = \\mathbf{w}_2^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_2 $  \n",
        "\n",
        "$ z^{[1](i)}_3 = \\mathbf{w}_3^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_3 $  \n",
        "\n",
        "$ z^{[1](i)}_4 = \\mathbf{w}_4^{[1]T}\\mathbf{x}^{(i)} + b^{[1]}_4 $  \n",
        "\n",
        "We can write the above set of equations in matrix form as under:  \n",
        "\n",
        "$\\mathbf{z}^{[1](i)} = \\begin{pmatrix}\n",
        "  {z}_1^{[1](i)} \\\\ {z}_2^{[1](i)} \\\\ z_3^{[1](i)} \\\\ {z}_{4}^{[1](i)}\n",
        " \\end{pmatrix} = \n",
        " \\begin{pmatrix}\n",
        "  \\mathbf{w}_1^{[1]T}\\mathbf x^{(i)} \\\\ \n",
        "  \\mathbf{w}_2^{[1]T}\\mathbf x^{(i)} \\\\ \n",
        "  \\mathbf{w}_3^{[1]T}\\mathbf x^{(i)} \\\\ \n",
        "  \\mathbf{w}_4^{[1]T}\\mathbf x^{(i)} \\end{pmatrix}\n",
        " +\\begin{pmatrix}\n",
        "  \\mathbf{b}_1 \\\\ \n",
        "  \\mathbf{b}_2 \\\\ \n",
        "  \\mathbf{b}_3 \\\\ \n",
        "  \\mathbf{b}_4 \\end{pmatrix}\n",
        "  $   "
      ],
      "metadata": {
        "id": "DdIJJauijp8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vectorized form is written as below    \n",
        "\n",
        "  $\\mathbf{z}^{[1](i)} = \\mathbf{W}^{[1]} \\mathbf{x}^{(i)} + \\mathbf{b}^{[1]}$\n",
        "This is the same as that of expression derived earlier."
      ],
      "metadata": {
        "id": "uEkq4awcqKWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbf{W}^{[1]} = \\begin{pmatrix}\n",
        "  \\mathbf{w}_1^{[1]T} \\\\ \\mathbf{w}_2^{[1]T} \\\\ \\mathbf{w}_3^{[1]T} \\\\ \\mathbf{w}_{4}^{[1]T}\n",
        " \\end{pmatrix} $"
      ],
      "metadata": {
        "id": "lmiC9eq5bDib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function \n",
        "We use $sigmoid()$ activation function when we need to calculate probability and the output is converted into 0 and 1 with the use of some threshold value.   \n",
        "\n",
        "We use $tanh()$ function for the hidden layer as it maps negative values with negative and zero with zero. The output of the $tanh()$ function ranges between $-1$ to $+1$. The shape of $tanh()$ is also s-shaped as the shape of $sigmoid()$ activation is but it is shfted to map 0 with 0.    \n",
        "$\\mathbf{a}^{[1](i)} = tanh(\\mathbf{z}^{[1](i)})$.   \n",
        "\n",
        "The $tanh(z)$ is given by $tanh(z)=\\large \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$. "
      ],
      "metadata": {
        "id": "DuQE5Igl0QRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above vectorized form of the equations are to be used for python implementation of the forward propagation of first hidden layer. We can represnt the input feature vector as the activation output of zeroth layer."
      ],
      "metadata": {
        "id": "278MDXTX0cNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Layer: Output Layer"
      ],
      "metadata": {
        "id": "EPpcSb-EVisk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-2  Node-1 \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-L2N1.png\" width=\"50%\">.  \n",
        "\n",
        "$ z^{[2](i)} = w_{11}^{[2]}a^{[1](i)}_1+w_{12}^{[2]}a^{[1](i)}_2+w_{13}^{[2]}a^{[1](i)}_3 + w_{14}^{[2]}a^{[1](i)}_4 + b^{[2]}_1 $   \n",
        "\n",
        "$ z^{[2](i)} = \\mathbf{w}^{[2]T}\\mathbf{a}^{[1](i)} + b^{[2]} $  \n",
        "\n",
        "$a^{[2](i)}=\\sigma (z^{[2](i)}) $\n"
      ],
      "metadata": {
        "id": "nE7-NZNbjbUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbf{z}^{[2](i)} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1](i)} + \\mathbf{b}^{[2]}$.   \n",
        "\n",
        "We will use $sigmoid()$ activation function for the output layer as we need the binary classification on the basis of the probability of output being true for give data.  \n",
        "\n",
        "\n",
        "$\\mathbf{a}^{[2](i)} = sigmoid(\\mathbf{z}^{[2](i)})$. \n",
        " \n",
        "The above vectorized form of the equations are to be used for python implementation of the forward propagation of output layer. "
      ],
      "metadata": {
        "id": "pC-9HVi11BYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation : One Training Example\n",
        "\n",
        "<img src=\"https://vijayonline.in/images/shallowNN-2LBP.png\">"
      ],
      "metadata": {
        "id": "i2Np7vHq0xWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us re-write the set of four vectorized equations developed above for hidden layer and output layer by the dropping `(i)` to represent single training example as:  \n",
        "\n",
        "**Hidden Layer**  \n",
        "\n",
        "$\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{a}^{[0]} + \\mathbf{b}^{[1]}$   \n",
        "\n",
        "$\\mathbf{a}^{[1]} = tanh(\\mathbf{z}^{[1]})$   \n",
        "\n",
        "**Output Layer**  \n",
        "\n",
        "$\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}$   \n",
        "\n",
        "$\\mathbf{a}^{[2]} = sigmoid(\\mathbf{z}^{[2]})$   \n",
        "\n",
        "These four vectorized equations, two for each layer are to be used for python implementation. "
      ],
      "metadata": {
        "id": "ZIS38Njp0CJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward Propagation to Compute Gradients: One Training Example"
      ],
      "metadata": {
        "id": "OehCiysZ1AvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward propagation starts with assumed values of the parameters. The parameters in the example of 2-Layer with 3 input features and 4 nodes in the hidden layer for the binary classification problem are $\\mathbf W^{[1]}, \\mathbf W^{[2]}, \\mathbf b^{[1]}, b^{[2]}$.   \n",
        "\n",
        "$ \\mathbf W^{[1]}.shape()=(4, 3)$  \n",
        "\n",
        "$ \\mathbf W^{[2]}.shape()=(1, 4)$  \n",
        "\n",
        "$ \\mathbf b^{[1]}.shape()=(4, 1)$  \n",
        "\n",
        "$ \\mathbf b^{[2]}.shape()=(1, 1)$  \n",
        "\n",
        "So, in all we have to optimize the 21 parameters for this network using the training data. For the purpose of explaining the method, we have assumed only one training example.   "
      ],
      "metadata": {
        "id": "VZzI58G3-LjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "We first need to calculate the loss. The loss function is assumed as:   \n",
        "\n",
        "$\\mathcal{L}(a, y) =  - y  \\log(a^{[2]}) - (1-y)  \\log(1-a^{[2]})$   \n",
        "\n",
        "The loss is to be minimized using garient descent optimization method. In this, we evaluate the gradient of the function and update the parameter till we reach the global minima. Following general update rules are applied:   \n",
        "\n",
        "$ \\mathbf w = \\mathbf w - \\alpha \\frac {\\partial \\mathcal L}{\\partial \\mathbf w}$  \n",
        "\n",
        "$ \\mathbf b = \\mathbf b - \\alpha \\frac {\\partial \\mathcal L}{\\partial \\mathbf b}$  \n",
        "\n",
        "Where,  \n",
        "        $ \\alpha$ : Learning Rate (0.0001, 0.001, 0.01...)\n"
      ],
      "metadata": {
        "id": "6ayViJSFFd1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Gradients for Output Layer\n",
        "The parameters of output layer are $w_{11}^{[2]}, w_{12}^{[2]}, w_{13}^{[2]}, w_{14}^{[2]}, b^{[2]}$. The gradients are computed in the same manner as derived in case of logistic regression as the sigmoid function is the activation function on output layer.     \n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial w_{11}^{[2]}}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}  \\frac{\\partial z^{[2]}}{\\partial w_{11}^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}\\small a^{[1]}_1=(a^{[2]}-y)a^{[1]}_1$\n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial w_{12}^{[2]}}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}  \\frac{\\partial z^{[2]}}{\\partial w_{12}^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}\\small a^{[1]}_2=(a^{[2]}-y)a^{[1]}_2$\n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial w_{13}^{[2]}}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}  \\frac{\\partial z^{[2]}}{\\partial w_{13}^{[2]}}=\\small(a^{[2]}-y)a^{[1]}_3$\n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial w_{14}^{[2]}}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}  \\frac{\\partial z^{[2]}}{\\partial w_{14}^{[2]}}=\\small(a^{[2]}-y)a^{[1]}_4$\n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}  \\frac{\\partial z^{[2]}}{\\partial b^{[2]}}=\\small(a^{[2]}-y)$"
      ],
      "metadata": {
        "id": "d3Xql2aZBaRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vectorized Gradient Equations for Output Layer***\n",
        "\n",
        "The first four equations of gradient descent gives us the gradient of the loss with respect to the weights. These equations can be written in matrix form as below:\n",
        "\n",
        "\n",
        "$\\large \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial w_{11}^{[2]}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w_{12}^{[2]}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w_{13}^{[2]}}\\\\ \\frac{\\partial \\mathcal{L}}{\\partial w_{14}^{[2]}}\n",
        "\\end{pmatrix}=\\small \\begin{pmatrix} (a^{[2]}-y)a^{[1]}_1 \\\\ (a^{[2]}-y)a^{[1]}_2 \\\\ (a^{[2]}-y)a^{[1]}_3 \\\\ (a^{[2]}-y)a^{[1]}_4 \\end{pmatrix}=(a^{[2]}-y)\\begin{pmatrix} a^{[1]}_1 \\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4 \\end{pmatrix}$\n",
        "\n",
        "The above matrix form can be written in vector form as:   \n",
        "\n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf w^{[2]}}=\\small (a^{[2]}-y)\\mathbf a^{[1]}$   \n",
        "\n",
        "$ \\mathbf w^{[2]} = \\mathbf w^{[2]} - \\alpha \\frac {\\partial \\mathcal L}{\\partial \\mathbf w^{[2]}}$   \n",
        "\n",
        "$ b = b - \\alpha \\frac {\\partial \\mathcal L}{\\partial b}$   \n",
        "\n",
        "The above three vectorized equations are to be used for python implementation."
      ],
      "metadata": {
        "id": "DRJyrfi7Jm5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Gradients for Hidden Layer\n",
        "The parameters of hidden layer are 12 elements of $\\mathbf W^{[1]}$ matrix and 4 elements of $\\mathbf b^{[1]}$ matrix. It is obvious that the shape $\\frac {\\partial L}{\\partial \\mathbf W^{[1]}}$ will be the same of $\\mathbf W^{[1]}$. Similarly, the shape $\\frac {\\partial L}{\\partial \\mathbf b^{[1]}}$ will be the same of $\\mathbf b^{[1]}$. Let us write the gradient matrix:   \n",
        "\n",
        "$\\large \\frac {\\partial L}{\\partial \\mathbf W^{[1]}} = \\begin{pmatrix}\n",
        "  \\frac {\\partial L}{\\partial w_{11}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{12}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{13}^{[1]}} \\\\ \n",
        "  \\frac {\\partial L}{\\partial w_{21}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{22}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{23}^{[1]}} \\\\\n",
        "  \\frac {\\partial L}{\\partial w_{31}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{32}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{33}^{[1]}} \\\\\n",
        "  \\frac {\\partial L}{\\partial w_{41}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{42}^{[1]}} & \n",
        "  \\frac {\\partial L}{\\partial w_{43}^{[1]}} \n",
        "  \\end{pmatrix};$\n",
        "  $\\large \\frac {\\partial L}{\\partial \\mathbf b^{[1]}} = \\begin{pmatrix}\n",
        "  \\frac {\\partial L}{\\partial b_{1}^{[1]}} \\\\ \n",
        "  \\frac {\\partial L}{\\partial b_{2}^{[1]}} \\\\ \n",
        "  \\frac {\\partial L}{\\partial b_{3}^{[1]}} \\\\ \n",
        "  \\frac {\\partial L}{\\partial b_{4}^{[1]}}\n",
        " \\end{pmatrix}$ \n"
      ],
      "metadata": {
        "id": "lu3rpU0VjYXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop the expressions for the each of the elements of above two matrices, let us consider first element $\\frac {\\partial L}{\\partial w_{11}^{[1]}}$ to build the concept.   \n",
        "<img src=\"https://vijayonline.in/images/shallowNN-2LBP2.png\" width=\"50%\">\n"
      ],
      "metadata": {
        "id": "MGcphNsum4ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{11}^{[1]}}=(\\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} )\n",
        "(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}})\n",
        "(\\frac{\\partial z^{[2]}}{\\partial a^{[1]}_{1}})\n",
        "(\\frac{\\partial a^{[1]}_{1}}{\\partial z^{[1]}_1})\n",
        "(\\frac{\\partial z^{[1]}_1}{\\partial w^{[1]}_{11}})$\n",
        "\n",
        "Let us evaluate these term one by one. We have already computed the first two terms in the gradient calculation of the output layer. So, let us club these two and compute the other as under:   \n",
        "$\\large \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}=\n",
        "\\frac{\\partial \\mathcal L}{\\partial a^{[2]}}\n",
        "\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\small=(a^{[2]}-y)$\n",
        "\n",
        "$\\large \\frac{\\partial z^{[2]}}{\\partial a^{[1]}_{1}}\\small=w^{[2]}_{11}$ : using $ z^{[2]} = w_{11}^{[2]}a^{[1]}_1+w_{12}^{[2]}a^{[1]}_2+w_{13}^{[2]}a^{[1]}_3 + w_{14}^{[2]}a^{[1]}_4 + b^{[2]}_1 $  \n",
        "\n",
        "$\\large \\frac{\\partial a^{[1]}_{1}}{\\partial z^{[1]}_1}\\small=g'(z^{[1]}_1)$\n",
        "\n",
        "$\\large \\frac{\\partial z^{[1]}_1}{\\partial w^{[1]}_{11}}\\small=x_1$ : using $ z^{[1]}_1 = w_{11}^{[1]}x_1+w_{12}^{[1]}x_2+w_{13}^{[1]}x_3 + b^{[1]}_1 $"
      ],
      "metadata": {
        "id": "kpWf5U-YpCB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining all the expression to get the desired gradient for updating the parameters associated with the first node of hidden layer:   \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{11}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{11}g'(z^{[1]}_1)x_1$\n",
        "  \n",
        "Similarly, we can write the other elements:  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{12}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{11}g'(z^{[1]}_1)x_2$  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{13}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{11}g'(z^{[1]}_1)x_3$\n"
      ],
      "metadata": {
        "id": "8_mAghKh2Opm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient to update the parameters associated with the second node of the hidden layer:  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{21}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{12}g'(z^{[1]}_2)x_1$\n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{22}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{12}g'(z^{[1]}_2)x_2$  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{23}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{12}g'(z^{[1]}_2)x_3$\n"
      ],
      "metadata": {
        "id": "s2ifDSc43Vw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient to update the parameters associated with the third node of the hidden layer:  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{31}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{13}g'(z^{[1]}_3)x_1$\n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{32}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{13}g'(z^{[1]}_3)x_2$  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{33}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{13}g'(z^{[1]}_3)x_3$\n"
      ],
      "metadata": {
        "id": "NcGD37Qr9o3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient to update the parameters associated with the fourth node of the hidden layer:  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{41}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{14}g'(z^{[1]}_4)x_1$\n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{42}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{14}g'(z^{[1]}_4)x_2$  \n",
        "\n",
        "$\\large \\frac {\\partial \\mathcal L}{\\partial w_{43}^{[1]}}\\small=(a^{[2]}-y)w^{[2]}_{14}g'(z^{[1]}_4)x_3$\n"
      ],
      "metadata": {
        "id": "m8cTSiLv9-ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation with m training examples  "
      ],
      "metadata": {
        "id": "b2QIysc4xpg3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBEGyO8gliBs"
      },
      "source": [
        "\n",
        "$ \\mathbf{Z}^{[1]} = \\begin{pmatrix}\n",
        "z^{[1](1)}_1 & z^{[1](2)}_1 & \\cdots & z^{[1](m)}_1\\\\\n",
        "z^{[1](1)}_2 & z^{[1](2)}_2 & \\cdots & z^{[1](m)}_2\\\\\n",
        "z^{[1](1)}_3 & z^{[1](2)}_3 & \\cdots & z^{[1](m)}_3\\\\\n",
        "z^{[1](1)}_4 & z^{[1](2)}_4 & \\cdots & z^{[1](m)}_4\n",
        "\\end{pmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV-gHfgWliBt"
      },
      "source": [
        "$ \\mathbf{Z}^{[1]} = \\begin{pmatrix}\n",
        "\\mathbf{z}^{[1](1)} & \\mathbf{z}^{[1](2)} & \\cdots & \\mathbf{z}^{[1](m)}\n",
        "\\end{pmatrix}$  \n",
        "\n",
        "$ \\mathbf{Z}^{[1]} = \\begin{pmatrix}\n",
        "\\mathbf{W}^{[1]} \\mathbf{x}^{(1)} + \\mathbf{b}^{[1]} & \\mathbf{W}^{[1]} \\mathbf{x}^{(2)} + \\mathbf{b}^{[1]} & \\cdots & \\mathbf{W}^{[1]} \\mathbf{x}^{(m)} + \\mathbf{b}^{[1]}\n",
        "\\end{pmatrix}$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdWWoG9SliBt"
      },
      "source": [
        "### Vectorized Linear Part\n",
        "$ \\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf X + \\mathbf{b}^{[1]}$  \n",
        "\n",
        "$ \\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{A^{[0]}} + \\mathbf{b}^{[1]}$ \n",
        "\n",
        "### Vectorized Activation Part\n",
        "$ \\mathbf{A}^{[1]}=\\sigma (\\mathbf{Z}^{[1]}) $    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Part\n",
        "\n",
        "$ \\mathbf{Z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{A}^{[1]} + \\mathbf{b}^{[2]}$ \n",
        "\n",
        "### Activation Part\n",
        "\n",
        "$ \\mathbf{A}^{[2]}=\\sigma (\\mathbf{Z}^{[2]}) $"
      ],
      "metadata": {
        "id": "qFzcL17OhAmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation Equations\n",
        "\n",
        "$\\mathbf Z^{[1]} =  \\mathbf W^{[1]} \\mathbf X + \\mathbf b^{[1]}$  \n",
        "\n",
        "$\\mathbf A^{[1]} = \\tanh(\\mathbf Z^{[1]})$  \n",
        "\n",
        "$\\mathbf Z^{[2]} = \\mathbf W^{[2]} \\mathbf A^{[1]} + b^{[2]}$  \n",
        "\n",
        "$ \\mathbf A^{[2]} = Sigmoid(\\mathbf Z^{[2]})$\n"
      ],
      "metadata": {
        "id": "QIZNQ37Pjs_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Function\n",
        "\n",
        "The $\\mathbf A^{[2]}$ contains $\\mathbf a^{[2](i)}$ for all examples. We can now compute the cost function as follows:\n",
        "\n",
        "$$ \\mathbf J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large [ \\small y^{(i)}\\log a^{[2] (i)} + (1-y^{(i)})\\log (1- a^{[2] (i)}) \\large ] $$\n",
        "\n",
        "**Parameters to be optimized are $\\mathbf W^{[1]}, \\mathbf W^{[2]}, \\mathbf b^{[1]}, \\mathbf b^{[2]}$**.   \n",
        "\n",
        "$ \\mathbf W^{[1]}.shape()=(n^{[1]}, n^{[0]})$  \n",
        "\n",
        "$ \\mathbf W^{[2]}.shape()=(n^{[2]}, n^{[1]})$  \n",
        "\n",
        "$ \\mathbf b^{[1]}.shape()=(n^{[1]}, 1)$  \n",
        "\n",
        "$ \\mathbf b^{[2]}.shape()=(n^{[2]}, 1)$  \n",
        "\n",
        "*We can use Gradient Descent for minimizing cost to get the optimized values of the parameters.*"
      ],
      "metadata": {
        "id": "uMFJ0yo1kyDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back Propagation to evaluate Gradients\n",
        "\n",
        "<img src=\"https://vijayonline.in/images/shallowNN-2LBP.png\">"
      ],
      "metadata": {
        "id": "oqXClGkyn7Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to evaluate the gradients of the cost function with respect to the parameters.   \n",
        "\n",
        "$\\large \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf W^{[2]} }, \\frac{\\partial \\mathbf {J} }{ \\partial b^{[2]}}, \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf W^{[1]}}, \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf b^{[1]} }$"
      ],
      "metadata": {
        "id": "M3lLe5ZVps5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf z^{[2]} } = \\frac{1}{m} (\\mathbf a^{[2]} - \\mathbf y)$$\n",
        "\n",
        "$$\\frac{\\partial \\mathbf {J} }{ \\partial W^{[2]} } = \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf z^{[2]} } \\mathbf a^{[1] T} $$\n",
        "\n",
        "$$ \\frac{\\partial \\mathbf {J} }{ \\partial b^{[2]} } = \\frac{\\partial \\mathbf {J} }{ \\partial \\mathbf z^{[2]}}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathbf {J} }{ \\partial z^{[1](i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$\n",
        "\n",
        "$$\\frac{\\partial \\mathbf {J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $$\n",
        "\n",
        "$$\\frac{\\partial \\mathbf {J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$$"
      ],
      "metadata": {
        "id": "mhAEND86pJQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using following notations python implementation:   \n",
        "\n",
        "$dW1 = \\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$;\n",
        "$db1 = \\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$;\n",
        "$dW2 = \\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$;\n",
        "$db2 = \\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$"
      ],
      "metadata": {
        "id": "WAzCVPJMqKXm"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "20NN2L.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}